{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml.html\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Funcionando Pega tudas as informações da página de detalhes.\n",
    "# \n",
    "# Pega link de detalhes.\n",
    "\n",
    "#url = 'https://portal.sitesagapesistemas.com.br/agape2/portal/ext/despesa/?alias=cmpinhao&p=iDespesa&base=670&tipo=empenho&ano=2024&i=95&a=detalhes'\n",
    "def pegar_detalhes(url):\n",
    "    data_hora_acesso = datetime.now()\n",
    "    HEADER = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"}\n",
    "    resp = requests.get(url, headers = HEADER, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    soup = bs(resp.text, 'lxml')\n",
    "\n",
    "\n",
    "    # Encontrar apos Identificação: numero, Natureza de Crédito, Data do Empenho, Tipo de Empenho\n",
    "    numero = soup.find('h2', string='Identificação').find_next('table').find_next('td', string='Número:').find_next('td').text\n",
    "    Natureza_de_Crédito = soup.find('h2', string='Identificação').find_next('table').find_next('td', string='Natureza de Crédito:').find_next('td').text\n",
    "    Data_do_Empenho = soup.find('h2', string='Identificação').find_next('table').find_next('td', string='Data do Empenho:').find_next('td').text\n",
    "    Tipo_de_Empenho= soup.find('h2', string='Identificação').find_next('table').find_next('td', string='Tipo de Empenho:').find_next('td').text\n",
    "\n",
    "    print('========== # Identificação: ==========')\n",
    "    print('Número:', numero)\n",
    "    print('Natureza de Crédito:', Natureza_de_Crédito)\n",
    "    print('Data do Empenho:', Data_do_Empenho)\n",
    "    print('Tipo de Empenho:', Tipo_de_Empenho)\n",
    "\n",
    "    df_identicacao = pd.DataFrame({'Número': [numero], 'Natureza de Crédito': [Natureza_de_Crédito], 'Data do Empenho': [Data_do_Empenho], 'Tipo de Empenho': [Tipo_de_Empenho]})\n",
    "    df_identicacao.head()\n",
    "\n",
    "\n",
    "\n",
    "    # ===============================================\n",
    "\n",
    "    # Encontrar apos Dotação: Poder, Função, Elemento de Despesa, Unid. Administradora, Subfunção, Subelemento, Unid. Orçamentária, Fonte de recurso, Projeto/Atividade\n",
    "\n",
    "    Poder = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Poder:').find_next('td').text\n",
    "    Função = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Função:').find_next('td').text\n",
    "    Elemento_de_Despesa = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Elemento de Despesa:').find_next('td').text\n",
    "    Unid_Administradora = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Unid. Administradora:').find_next('td').text\n",
    "    Subfunção = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Subfunção:').find_next('td').text\n",
    "    Subelemento = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Subelemento:').find_next('td').text\n",
    "    Unid_Orçamentária = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Unid. Orçamentária:').find_next('td').text\n",
    "    Fonte_de_recurso = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Fonte de recurso:').find_next('td').text\n",
    "    Projeto_Atividade = soup.find('h2', string='Dotação').find_next('table').find_next('td', string='Projeto/Atividade').find_next('td').text\n",
    "\n",
    "\n",
    "\n",
    "    # ===============================================\n",
    "\n",
    "    # Encontrar apos Outras Informações: Categorias de base legal\n",
    "\n",
    "    Categorias_base_legal = soup.find('h2', string='Outras Informações').find_next('table').find_next('td', string='Categorias de base legal').find_next('td').text\n",
    "\n",
    "\n",
    "    # ==============================================\n",
    "\n",
    "    # Encontrar apos Financeiro: Alteração, Empenhado, Liquidado, Pago, Histórico\n",
    "\n",
    "    Alteracao = soup.find('h2', string='Financeiro').find_next('table').find_next('td', string='Alteração').find_next('td').text\n",
    "    Empenhado = soup.find('h2', string='Financeiro').find_next('table').find_next('td', string='Empenhado').find_next('td').text\n",
    "    Liquidado = soup.find('h2', string='Financeiro').find_next('table').find_next('td', string='Liquidado').find_next('td').text\n",
    "    Pago = soup.find('h2', string='Financeiro').find_next('table').find_next('td', string='Pago').find_next('td').text\n",
    "    Historico = soup.find('h2', string='Financeiro').find_next('table').find_next('td', string='Histórico').find_next('td').text\n",
    "\n",
    "    # ==============================================\n",
    "\n",
    "    # Encontrar apos Item(ns): \n",
    "\n",
    "\n",
    "    def pegar_nome_das_colunas(soup):\n",
    "        column_names = [x.text for x in soup.find_next('tr') if x.text != '\\n']\n",
    "        return column_names\n",
    "\n",
    "\n",
    "    def pegar_dados(soup):\n",
    "        item_list = []\n",
    "        itens = [item.text.split('\\n') for item in soup.find('h2', string='Item(ns)').find_next('table') if item.text != '\\n']\n",
    "        itens = [item[1:-1] for item in itens]\n",
    "        return itens[1:]\n",
    "\n",
    "    table = soup.find('h2', string='Item(ns)').find_next('table')\n",
    "    colunas = pegar_nome_das_colunas(table)\n",
    "    linhas = pegar_dados(soup)\n",
    "\n",
    "    itens = [colunas, linhas]\n",
    "\n",
    "    # print('========== # Dotação: ==========')\n",
    "    # print('Poder:', Poder)\n",
    "    # print('Função:', Função)\n",
    "    # print('Elemento de Despesa:', Elemento_de_Despesa)\n",
    "    # print('Unid. Administradora:', Unid_Administradora)\n",
    "    # print('Subfunção:', Subfunção)\n",
    "    # print('Subelemento:', Subelemento)\n",
    "    # print('Unid. Orçamentária:', Unid_Orçamentária)\n",
    "    # print('Fonte de recurso:', Fonte_de_recurso)\n",
    "    # print('Projeto/Atividade:', Projeto_Atividade)\n",
    "    # print('========== # Outras Informações: ==========')\n",
    "    # print('Categorias de base legal:', Categorias_base_legal)\n",
    "    # print('========== # Financeiro: ==========')\n",
    "    # print('Alteração:', Alteracao)\n",
    "    # print('Empenhado:', Empenhado)\n",
    "    # print('Liquidado:', Liquidado)\n",
    "    # print('Pago:', Pago)\n",
    "    # print('Histórico:', Historico)\n",
    "    # print('========== # Item(ns): ==========')\n",
    "    # print(itens)\n",
    "\n",
    "\n",
    "    df_dotação = pd.DataFrame({'Poder': [Poder], 'Função': [Função], 'Elemento de Despesa': [Elemento_de_Despesa], 'Unid. Administradora': [Unid_Administradora], 'Subfunção': [Subfunção], 'Subelemento': [Subelemento], 'Unid. Orçamentária': [Unid_Orçamentária], 'Fonte de recurso': [Fonte_de_recurso], 'Projeto/Atividade': [Projeto_Atividade], 'Categorias de base legal': [Categorias_base_legal], 'Alteração': [Alteracao], 'Empenhado': [Empenhado], 'Liquidado': [Liquidado], 'Pago': [Pago], 'Histórico': [Historico], 'Item(ns)': [itens]})\n",
    "    return df_dotação\n",
    "#url = 'https://portal.sitesagapesistemas.com.br/agape2/portal/ext/despesa/?alias=cmpinhao&p=iDespesa&base=670&tipo=empenho&ano=2024&i=95&a=detalhes'\n",
    "#dicionario = pegar_detalhes(url)\n",
    "\n",
    "\n",
    "def raspar_dados(pg, mes, ano, datade, dataate):\n",
    "    url = f'https://portal.sitesagapesistemas.com.br/agape2/portal/ext/despesa/?tipo_arq=&alias=cmpinhao&p=iDespesa&filtro=1&pg={pg}&mes={mes}&ano={ano}&datade={datade}&dataate={dataate}&tipo=empenho&credor=&classificacao=&documento=&v='\n",
    "    HEADER = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        resp = requests.get(url, headers = HEADER, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        raise SystemExit(err)\n",
    "    soup = bs(resp.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def pegar_nome_das_colunas(soup):\n",
    "    table_tag = soup.table\n",
    "    column_names = [x.text for x in table_tag.thead.tr.find_all('strong') if x.text != '']\n",
    "    column_names = column_names[:8]\n",
    "    column_names.append('link_Detalhes')\n",
    "    return column_names\n",
    "\n",
    "def pegar_dados(soup):\n",
    "    despesas = soup.select('table > tbody > tr ')\n",
    "    rows = []\n",
    "    for despesa in despesas:\n",
    "        list_despesas=[]\n",
    "        list_despesas = [x for x in despesa.stripped_strings]\n",
    "        list_despesas = list_despesas[1:9]\n",
    "        links_alias = despesa.find_all('a', href=lambda href: href and href.startswith('?alias'))\n",
    "        links_alias = 'https://portal.sitesagapesistemas.com.br/agape2/portal/ext/despesa/'+links_alias[0].get('href').replace('amp;', '')\n",
    "        list_despesas.append(links_alias)\n",
    "        rows.append(list_despesas)\n",
    "    return rows\n",
    "\n",
    "def pegar_ultima_pagina(soup):\n",
    "    last_page = max([int(x.text) for x in soup.select('div[id=\"paginacao\"] ul[class=\"pagination pointer\"] li') if x.text.isdigit()])\n",
    "    return last_page\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(pg, mes, ano, datade, dataate):\n",
    "    soup = raspar_dados(pg, mes, ano, datade, dataate)\n",
    "    column_names = pegar_nome_das_colunas(soup)\n",
    "    rows = pegar_dados(soup )\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "    return df\n",
    "\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    pg = '1'\n",
    "    mes = '09'\n",
    "    ano = '2024'\n",
    "    datade = F'01%2F01%2F{ano}'\n",
    "    dataate = F'31%2F12%2F{ano}'\n",
    "    df = main(pg, mes, ano, datade, dataate)\n",
    "    print(df.shape)\n",
    "    #display(df.head())\n",
    "    #buscar delahes para cada linha e ao final fazer a concatenação \n",
    "    novo_df = pd.DataFrame()\n",
    "    # Iterar pelas linhas do DataFrame\n",
    "    for idx, linha in df.iterrows():\n",
    "        time.sleep(random.randint(5, 10))  # Aguarda um tempo aleatório entre 1 e 3 segundos\n",
    "        url = linha[-1]  # A última coluna deve conter a URL\n",
    "        print(f\"Processando URL: {url}\")\n",
    "        \n",
    "        # Chama a função pegar_detalhes para obter os detalhes da URL\n",
    "        df_temp = pegar_detalhes(url)\n",
    "        \n",
    "        # Verifica se df_temp é um DataFrame e possui dados antes de concatenar\n",
    "        if df_temp is not None and not df_temp.empty:\n",
    "            # Converte a linha original em um DataFrame (1 linha, várias colunas)\n",
    "            df_linha_original = pd.DataFrame([linha])\n",
    "            \n",
    "            # Concatenar horizontalmente (lado a lado)\n",
    "            df_expandido = pd.concat([df_linha_original.reset_index(drop=True), df_temp.reset_index(drop=True)], axis=1)\n",
    "            \n",
    "            # Adicionar a linha combinada ao novo DataFrame\n",
    "            novo_df = pd.concat([novo_df, df_expandido], ignore_index=True)\n",
    "            print(novo_df.head())\n",
    "    # Exibe o novo DataFrame com as linhas combinadas\n",
    "    print(novo_df.head())'''\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
